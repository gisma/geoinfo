---
title: "Part 2: Analyzing massive Sentinel Data amounts with R, gdalcubes, and STAC"
author:
- Marius Appel
- adapted by Chris Reudenbach
date: "Sept. 1, 2021"
link-citations: yes
output: 
  html_document:
    toc: true
    toc_float:  
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
    theme: flatly
editor_options: 
  chunk_output_type: console
---

```{r setup , include = FALSE}
tutorialDir = paste0(usethis::proj_get(), "/data/tutorial/")
figtrim <- function(path) {
  img <- magick::image_trim(magick::image_read(path))
  magick::image_write(img, path)
  path
}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(out.width = "100%")
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(dev = "png")
knitr::opts_chunk$set(fig.process = figtrim)
knitr::opts_chunk$set(fig.width = 15, fig.height = 7.5)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(root.dir = tutorialDir)
knitr::opts_chunk$set(fig.path=paste0(tutorialDir,'figs/'))
```


# Introduction

This tutorial demonstrates how we can access and process Sentinel-2 data in the cloud using the R packages [`@rstac`](https://cran.r-project.org/package=rstac) and [`@gdalcubes`](https://cran.r-project.org/package=gdalcubes).

Other packages used in this tutorial include [`stars`](https://cran.r-project.org/package=stars)[@stars] and [`@tmap`](https://cran.r-project.org/package=tmap) for creating interactive maps, [`@sf`](https://cran.r-project.org/package=sf) for processing vector data, and [`@colorspace`](https://cran.r-project.org/package=colorspace)[@colorspace] for visualizations with accessible colors. To exemplarify easy to use sources of geometry data the packages [`@rnaturalearth`](https://cran.r-project.org/package=rnaturalearth) and via common download European Authority geometries are used. 

# Retrieve some interesting reference geometries

```{r state}
library(sf)
library(rnaturalearth)
library(tidyverse)
library(downloader)
library(tmap)
library(gdalcubes)


# define names
state_name = "Hessen"
municip_name = "Lahntal"

#- get country/state boundaries from rnaturalearth package
country <- ne_states(country = 'germany',returnclass='sf')
state= country %>% filter(name == state_name)

#-  geometry data of the municipal areas  (Bundesamt für Geodäsie und Kartographie)

# https://gdz.bkg.bund.de/index.php/default/open-data/verwaltungsgebiete-1-250-000-mit-einwohnerzahlen-ebenen-stand-31-12-vg250-ew-ebenen-31-12.html
if (!file.exists(paste0(tutorialDir,"municipal.zip"))){
  downloader::download(url ="https://daten.gdz.bkg.bund.de/produkte/vg/vg250-ew_ebenen_1231/aktuell/vg250-ew_12-31.tm32.shape.ebenen.zip", destfile = paste0(tutorialDir,"municipal.zip"))
  # Entpackt werden nur die Gemeindegeometrien
  unzip(zipfile = paste0(tutorialDir,"municipal.zip"),
        files = c("vg250-ew_12-31.tm32.shape.ebenen/vg250-ew_ebenen_1231/VG250_GEM.shp",
                  "vg250-ew_12-31.tm32.shape.ebenen/vg250-ew_ebenen_1231/VG250_GEM.dbf",
                  "vg250-ew_12-31.tm32.shape.ebenen/vg250-ew_ebenen_1231/VG250_GEM.shx",
                  "vg250-ew_12-31.tm32.shape.ebenen/vg250-ew_ebenen_1231/VG250_GEM.prj"),
        exdir = paste0(tutorialDir,"municipal/"),
        junkpaths = TRUE)
}
municipal = st_read(paste0(tutorialDir,"municipal/VG250_GEM.shp"))



```

We aim at generating a cloud-free composite image of our study area using the `rstac` package to find suitable Sentinel-2 images. However, to use the `bbox` argument of the corresponding function `stac_search()` for spatial filtering, we first need to derive and transform the bounding box to latitude / longitude (WGS84) values, for which we use the `st_bbox()` and `st_transform()` functions. In addition we adapt the projection of the referencing vector objects to all later needs.Please note to project to three different CRS is for convinience and clarity in this turorial and somewhat cumbersome.

```{r geometries}
# project the data for state
state_4326 = st_transform(state,crs = 4326)
state_3035 = st_transform(state,crs = 3035)
state_32632 = st_transform(state,crs = 32632)


municipal_4326 = st_transform(municipal, 4326) %>% filter(GEN == municip_name)
municipal_3035 = st_transform(municipal,crs = 3035)
municipal_32632 = st_transform(municipal,crs = 32632)

# mapping the extents and boundaries of the choosen geometries
tmap_mode("view")

tm_shape(st_as_sfc(st_bbox(state)),check.and.fix = TRUE) + tm_polygons(alpha = 0.1,col = "red")  + 
  tm_shape(municipal_4326) +   tm_polygons(col="green") +
  tm_shape(municipal) +   tm_polygons(alpha = 0.01, check.and.fix = TRUE) +
  tm_shape(state) +   tm_polygons(alpha = 0.1, col="magenta")
```




# Time series analysis

This example shows how complex times series methods from external R packages can be applied in cloud computing environments using [`rstac`](https://cran.r-project.org/package=rstac) [@rstac] and [`gdalcubes`](https://cran.r-project.org/package=gdalcubes) [@gdalcubes]. We will use the [`bfast` R package](https://cran.r-project.org/package=bfast)[@verbesselt2010] containing unsupervised change detection methods identifying structural breakpoints in vegetation index time series. Specifically, we will use the `bfastmonitor()` function to monitor changes on a time series of Sentinel-2 imagery.

Compared to the first example, our study area is rather small, covering a small forest area in the southeast of Berlin. The area of interest is again available as a polygon in a GeoPackage file `gruenheide_forest.gpkg`, which for example can be visualized in a map using the [`tmap` package](https://cran.r-project.org/package=tmap)[@tmap] package.


## Querying images with `rstac`

Using the `rstac` package, we first request all available images from 2016 to 2020 that intersect with our region of interest. Here, since the polygon has WGS84 as CRS, we do **not** need to transform the bounding box before using the `stac_search()` function.

```{r stac2}
library(rstac)
s = stac("https://earth-search.aws.element84.com/v0")
items <- s |>
    stac_search(collections = "sentinel-s2-l2a-cogs",
                bbox = c(st_bbox(municipal_4326)["xmin"],
                         st_bbox(municipal_4326)["ymin"],
                         st_bbox(municipal_4326)["xmax"],
                         st_bbox(municipal_4326)["ymax"]), 
                datetime = "2016-01-01/2021-12-31",
                limit = 1000) |>
    post_request() 
items
# Date and time of first and last images
range(sapply(items$features, function(x) {x$properties$datetime}))
```

This gives us 516 images recorded between Nov. 2016 and Dec. 2021.

## Creating a monthly Sentinel-2 data cube

To build a regular monthly data cube, we again need to create a gdalcubes image collection from the STAC query result. Notice that to include the `SCL` band containing per-pixel quality flags (classification as clouds, cloud-shadows, and others), we need to explicitly list the names of the assets. We furthermore ignore images with 75% or more cloud coverage.

```{r add_cube_assets}
library(gdalcubes)
assets = c("B01","B02","B03","B04","B05","B06", "B07","B08","B8A","B09","B11","SCL")
s2_collection = stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[["eo:cloud_cover"]] < 75}) 
s2_collection
```

The result still contains 253 images, from which we can now create a data cube. We use the transformed (UTM) bounding box of our polygon as spatial extent, 10 meter spatial resolution, bilinear spatial resampling and derive monthly median values for all pixel values from multiple images within a month, if available. Notice that we add 100m to each side of the cube.

```{r create_cubeview}
v = cube_view(srs = "EPSG:32632", 
              dx = 10, 
              dy = 10, 
              dt = "P1M",  
              aggregation = "median", 
              extent = list(t0 = "2016-01",
                            t1 = "2021-12", 
                            left = st_bbox(municipal_32632)["xmin"]-100, 
                            right = st_bbox(municipal_32632)["xmax"]+100,
                            bottom = st_bbox(municipal_32632)["ymin"]-100, 
                            top = st_bbox(municipal_32632)["ymax"]+100),
              resampling = "bilinear")
v
```

Next, we create a data cube, subset the red and near infrared bands and crop by our polygon, which simply sets pixel values outside of the polygon to NA. Afterwards we save the data cube as a single netCDF file. Notice that this is not neccessary but storing intermediate results makes debugging sometimes easier, especially if the methods applied afterwards are computationally intensive.


```{r write_ncdf}
if (!file.exists(paste0(tutorialDir,"lahntal.nc"))){
  s2.mask = image_mask("SCL", values = c(3,8,9))
  gdalcubes_options(threads = 8, ncdf_compression_level = 5)
  raster_cube(s2_collection, v, mask = s2.mask) |>
    select_bands(c("B04","B08")) |>
    filter_geom(lahn$geometry) |>
    write_ncdf(paste0(tutorialDir,"lahntal.nc"))}
```

## Reduction over space and time

To get an overview of the data, we can compute simple summary statistics (applying reducer functions) over dimensions. Below, we derive minimum, maximum, and mean monthly NDVI values over all pixel time series.

```{r municipal_ndvi}
library(colorspace)
ndvi.col = function(n) {
  rev(sequential_hcl(n, "Green-Yellow"))
}
ncdf_cube(paste0(tutorialDir,"lahntal.nc")) |>
  apply_pixel("(B08-B04)/(B08+B04)", "NDVI") |>
  reduce_time("min(NDVI)", "max(NDVI)", "mean(NDVI)") |>
  plot(key.pos = 1, zlim=c(-0.2,1), col = ndvi.col, nbreaks = 19)
```

Possible reducers include `"min"`, `"mean"`, `"median"`, `"max"`, `"count"` (count non-missing values), `"sum"`, `"var"` (variance), and `"sd"` (standard deviation). Reducer expressions are always given as a string starting with the reducer name followed by the band name in parentheses. Notice that it is possible to mix reducers and bands.

The `"count"` reducer is often very useful to get an initial understanding of an image collection:

```{r count}
ncdf_cube(paste0(tutorialDir,"lahntal.nc")) |>
  reduce_time("count(B04)") |>
  plot(key.pos = 1, zlim=c(0,60), col = viridis::viridis, nbreaks = 10)
```

We can see that most time series contain valid observations for 40-50 months, which should be sufficient for our example. Similarly, it is also possible to reduce over space, leading to summary time series.

```{r timeseries_ndvi_municipal}
ncdf_cube(paste0(tutorialDir,"lahntal.nc")) |>
  apply_pixel("(B08-B04)/(B08+B04)", "NDVI") |>
  reduce_space("min(NDVI)", "max(NDVI)", "mean(NDVI)") |>
  plot(join.timeseries = TRUE)
```

## Applying bfastmonitor as a user-defined reducer function

To apply a more complex time series method such as `bfastmonitor()`, the data cube operations below allow to provide custom user-defined R functions instead of string expressions, which translate to built-in reducers. It is very important that these functions receive arrays as input and must return arrays as output, too. Depending on the operation, the dimensionality of the arrays is different:

+----------------+---------------------------------------------------------+------------------------------------+
| Operator       | Input                                                   | Output                             |
+================+=========================================================+====================================+
| `apply_pixel`  | Vector of band values for one pixel                     | Vector of band values of one pixel |
+----------------+---------------------------------------------------------+------------------------------------+
| `reduce_time`  | Multi-band time series as a matrix                      | Vector of band values              |
+----------------+---------------------------------------------------------+------------------------------------+
| `reduce_space` | Three-dimensional array with dimensions bands, x, and y | Vector of band values              |
+----------------+---------------------------------------------------------+------------------------------------+
| `apply_time`   | Multi-band time series as a matrix                      | Multi-band time series as a matrix |
+----------------+---------------------------------------------------------+------------------------------------+

There is no limit in what we can do in the provided R function, but we must take care of a few things:

1.  The reducer function is executed in a new R process without access to the current workspace. It is not possible to access variables defined outside of the function and packages must be loaded **within** the function.

2.  The reducer function **must** always return a vector with the same length (for all time series).

3.  It is a good idea to think about `NA` values, i.e. you should check whether the complete time series is `NA`, and that missing values do not produce errors.

Another possibility to apply R functions to data cubes is of course to convert data cubes to `stars` objects and use the `stars` package.

In our example, `bfastmonitor()` returns change date and change magnitude values per time series and we can use `reduce_time()`. The script below calculates the [kNDVI](https://advances.sciencemag.org/content/7/9/eabc7447), applies `bfastmonitor()`, and properly handles errors e.g. due to missing data with `tryCatch()`. Finally, resulting change dates and magnitudes for all pixel time series are written to disk as a netCDF file.

```{r calc_bfast }
if (!file.exists(paste0(tutorialDir,"bf_results.nc"))) {
  gdalcubes_options(threads = 8)
  system.time(
    ncdf_cube(paste0(tutorialDir,"lahntal.nc")) |>
      reduce_time(names = c("change_date", "change_magnitude"), FUN = function(x) {
        knr <- exp(-((x["B08",]/10000)-(x["B04",]/10000))^2/(2))
        kndvi <- (1-knr) / (1+knr)   
        if (all(is.na(kndvi))) {
          return(c(NA,NA))
        }
        kndvi_ts = ts(kndvi, start = c(2016, 1), frequency = 12)
        library(bfast)
        tryCatch({
          result = bfastmonitor(kndvi_ts, start = c(2020,1), 
                                history = "all", level = 0.01)
          return(c(result$breakpoint, result$magnitude))
        }, error = function(x) {
          return(c(NA,NA))
        })
      }) |>
      write_ncdf(paste0(tutorialDir,"bf_results.nc")))
}
```

Running `bfastmonitor()` is computationally expensive. However, since the data is located in the cloud anyway, it would be obvious to launch one of the more powerful machine instance types with many processors. Parallelization within one instance can be controlled entirely by `gdalcubes` using `gdalcubes_options()`.

## Results

To visualize the change detection results, we load the resulting netCDF file, convert it to a `stars` object, and finally use the `tmap` package to create an interactive map to visualize the change date.

```{r bfast_change}
tutorialDir = paste0(usethis::proj_get(), "/data/tutorial/")
figtrim <- function(path) {
  img <- magick::image_trim(magick::image_read(path))
  magick::image_write(img, path)
  path
}
library(stars)
library(gdalcubes)
library(tmap)
ncdf_cube(paste0(tutorialDir,"bf_results.nc")) |>
  st_as_stars() -> x
m1 = tm_shape(x[1]) + tm_raster()  +tm_layout(
          legend.title.size = 1,
          legend.text.size = 0.6,
          legend.outside = T,
          legend.outside.position =  c("left","bottom"))
m2 =  tm_shape(x[2])  + tm_raster()  +tm_layout(
          legend.title.size = 1,
          legend.text.size = 0.6,
          legend.outside = T,
          legend.outside.position =  c("left","bottom"))
tmap_arrange(m1,m2)
```

The result certainly needs some postprocessing to understand types of changes and to identify false positives.

## Summary

This example has shown, how more complex time series methods as from external R packages can be applied on data cubes. For computationally intensive methods, it is in many cases useful to store intermediate results by combining `write_ncdf()` and `ncdf_cube()`. 


